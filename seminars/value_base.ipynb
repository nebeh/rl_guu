{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Методы основанные на полезности\n",
    "В любых методах RL цель заключается в нахождении оптимальной стратегии (политики). Можно это делать используя разные подходы. В случае методов основанных на полезностях, мы сначала находим полезность всех состояний. Оптимальная стратегия для любого состояния - двигаться в сторону где полезность выше. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как работаю методы основанные на полезности состояния на упрощенном учебном примере.\n",
    "\n",
    "Среда - клеточный мир размером 1хN (на картинке ниже, среда размером 1x5)\n",
    "\n",
    "![title](environment.png)\n",
    "\n",
    "Цель: достижение терминального состояния из любой клетки за минимальное количество действий.\n",
    "\n",
    "Для решения задачи очень удобно реализовать среду в виде класса (это стандарт в RL) с методом step, который принимает на вход действие агента, на выходе возвращает новое состояние среды и награду за текущее действие. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Определение среды 1xN клеточного мира\n",
    "class GridWorld1xN:\n",
    "    def __init__(self, N,start): # N - количество клеток мира, start - номер клетки в которой находится агент в начале эпизода \n",
    "        self.N = N\n",
    "        self.start_state = start  # С какой клетки стартуем\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.agent_pos = self.start_state\n",
    "        self.done = True\n",
    "        return self.agent_pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Действие: 0 - влево, 1 - вправо\n",
    "        if action == 0:\n",
    "            self.agent_pos = max(0, self.agent_pos - 1)\n",
    "            reward = -1\n",
    "        elif action == 1:\n",
    "            self.agent_pos = self.agent_pos + 1\n",
    "            if self.agent_pos == self.N - 1: # если оказались в терминальном состоянии\n",
    "                self.done = False\n",
    "\n",
    "            reward = -1.0 if self.done else 1 \n",
    "        else:\n",
    "            raise ValueError(\"Некорректное действие\")\n",
    "        \n",
    "    
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Монте Карло\n",
    "Идея Монте Карло заключается в том, что вместо честного матожидания по rewards \n",
    "$$ V^{\\pi}(s) = E_{\\pi}(R)$$\n",
    "Мы делаем оценку мат ожидания \"семплируя\" из раcпределения $\\pi$ на каждом шаге, пока не дошли до конца эпизода. В итоге, оценка стоимости состояния:\n",
    "$$ V = \\frac{\\sum_i^n R_i}{n} $$\n",
    "где $R_i = r_0 + \\gamma r_1 + \\gamma^2 r_2 + ...$ - реализация переходов агента из начального в терминальное состояние в соответствии со стратегией $\\pi$  \n",
    "Оценка V будет тем лучше, чем больше мы сделаем симуляций\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-86.97 , -85.195, -82.   , -78.86 , -73.515, -56.375, -46.765,\n",
       "       -32.485, -12.965,   0.   ])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPISODE = 200\n",
    "NUM_GRID = 10\n",
    "V = np.zeros(NUM_GRID)\n",
    "for k in range(NUM_GRID-1):\n",
    "    REWARD = []\n",
    "    for i in range (NUM_EPISODE):\n",
    "        R = 0      \n",
    "        env = GridWorld1xN(NUM_GRID,k)\n",
    "        while env.done:\n",
    "            action = np.random.randint(2)\n",
    "            reward = env.step(action)\n",
    "            R += reward\n",
    "        REWARD.append(R)\n",
    "    V[k] = np.mean(REWARD)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат: мы видим что стоимость состояний увеличивается слева направо. Таким образом мы правильно нашли оптимальную стратегию (в этом, очень простом случае, мы ее знаем). Но для этого нам понадобилось порядка 200 симуляций для каждой клетки.  \n",
    "Попробуем другие варианты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(0) обучение\n",
    "В TD обучении мы предполагаем что среда удовлетворяет условию Маркова (будущее зависит только от настоящего). В этом методе мы оцениваем текущее состояние через ближайшее будущее состояние в соостветствии с выбранной стратегией.  \n",
    "$$ V^{\\pi}(s) = r + \\gamma V(s')$$\n",
    "Если наша стратегия стохастическая, мы не знаем заранее в какое состояние попадем. В метода TD мы семплируем из распределения стратегии и таким образом реализуем конкретное $V(s')$\n",
    "Обновление $V(s)$:\n",
    "$$ V_{t+1}(s) =V_{t}(s) + \\alpha( r + \\gamma V_t(s') - V_t(s)) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.73210404, -8.71952056, -8.6657751 , -8.4751502 , -8.19206809,\n",
       "       -8.11332224, -7.13912007, -5.05189462, -0.47076087,  0.        ])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.2\n",
    "NUM_EPISODE = 100\n",
    "NUM_GRID = 10\n",
    "V = np.zeros(NUM_GRID)\n",
    "gamma = 0.9\n",
    "for t in range(NUM_EPISODE):    \n",
    "    for k in range(NUM_GRID-1):\n",
    "        env = GridWorld1xN(NUM_GRID,k)\n",
    "        action = np.random.randint(2)\n",
    "        reward = env.step(action)\n",
    "        V[k] = V[k] + alpha*(reward+gamma*V[env.agent_pos]-V[k])\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Уравнение Беллмана\n",
    "Отличие TD от уравнения Беллмана заключается в том, что для оценки текущей стоимости мы берем мат ожидание по будущим состояниям (которые возможны на следующем шаге)\n",
    "$$ V^{\\pi}(s) = E_{\\pi}[r + \\gamma V(s')]$$\n",
    "Если стратегия равномерная, это означает, что мы берем среднее по всем V на следующем шаге. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87792277, 0.87672091, 0.87073742, 0.8573505 , 0.83185751,\n",
       "       0.78636559, 0.7080041 , 0.57608515, 0.35759343, 0.        ])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.2\n",
    "NUM_EPISODE = 100\n",
    "NUM_GRID = 10\n",
    "V = np.zeros(NUM_GRID)\n",
    "for t in range(NUM_EPISODE):    \n",
    "    for k in range(NUM_GRID-1):\n",
    "        env = GridWorld1xN(NUM_GRID,k)\n",
    "        reward_0 = env.step(0) # награда при движении влево\n",
    "        reward_1 = env.step(1) # награда при движении вправо\n",
    "        if k == 0:\n",
    "            V_left = V[0]\n",
    "        else:\n",
    "            V_left = V[k-1]\n",
    "        V_right = V[k+1]\n",
    "        V_pi = (reward_0 + reward_1 + V_left + V_right)/2\n",
    "        \n",
    "\n",
    "        V[k] = V[k] + alpha*(reward+gamma*V_pi-V[k])\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax\n",
    "Во всех методах выше, мы неявно обучали стратегию (начинали с равномерной и в итоге пришли к оптимальной). Можем сделать это явно, на каждом шаге меняя стратегию, выбирая следующее действие с распределением основанным на стоимости следующих состояний. Иначе говоря, чем выше стоимость состояния тем вероятность его выбора будет выше.  \n",
    "В нашем клеточном мире выбор следующего состояния будем семплироваться из распределения (стратегии):\n",
    "$$ \\pi = softmax([V[k-1],V[k+1]])$$\n",
    "В коде ниже можем видеть что стратегия будет меняться по мере обновления $V(s_i)$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x1,x2):\n",
    "    e1 = np.exp(x1)\n",
    "    e2 = np.exp(x2)\n",
    "    return [e1/(e1+e2),e2/(e1+e2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.64034033, -5.61074901, -5.47559225, -4.82822643, -3.87636774,\n",
       "       -3.19825017, -1.9910103 , -0.93944755, -0.00645988,  0.        ])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.2\n",
    "NUM_EPISODE = 40\n",
    "NUM_GRID = 10\n",
    "V = np.zeros(NUM_GRID)\n",
    "gamma = 0.9\n",
    "for t in range(NUM_EPISODE):    \n",
    "    for k in range(NUM_GRID-1):\n",
    "        env = GridWorld1xN(NUM_GRID,k)\n",
    "        if k == 0:\n",
    "            a = softmax(V[0],V[1])\n",
    "            action = np.random.choice([0,1], p = a)\n",
    "            V_next = V[k+action]\n",
    "        else:\n",
    "            a = softmax(V[k-1],V[k+1])\n",
    "            action = np.random.choice([0,1], p = a)\n",
    "            if action == 0:\n",
    "                V_next = V[k-1]\n",
    "            else:\n",
    "                V_next = V[k+1]\n",
    "\n",
    "        reward = env.step(action)\n",
    "        V[k] = V[k] + alpha*(reward+gamma*V_next-V[k])\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Уравнение оптимальности Беллмана\n",
    "$$ V(s) = r + \\gamma \\max(V(s'))$$\n",
    "В этом случае мы обновляем текущее значение стоимости выбирая для обновления наилушее возможное состояние в будущем. Обратите внимание, что это off policy метод, в нем мы не обучаем стратегию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.17217618, -5.13511554, -4.93894268, -4.6107066 , -4.15328142,\n",
       "       -3.57730503, -2.89550873, -2.11662266, -1.24298888, -0.27023588])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.2\n",
    "NUM_EPISODE = 40\n",
    "NUM_GRID = 10\n",
    "V = np.random.randn(NUM_GRID)\n",
    "for t in range(NUM_EPISODE):    \n",
    "    for k in range(NUM_GRID-1):\n",
    "        reward = -1\n",
    "        if k == 0:\n",
    "            V_max = np.max([V[0],V[1]])\n",
    "        else:\n",
    "            V_max = np.max([V[k-1],V[k+1]])\n",
    "        \n",
    "        if k == NUM_GRID - 1:\n",
    "            if V[k-1]<V[k+1]:\n",
    "                reward = 1\n",
    "        \n",
    "\n",
    "        V[k] = V[k] + alpha*(reward+gamma*V_max-V[k])\n",
    "V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
